the main type of insurance in the united states is private insurance provided at work to an employee.
the financial implications of getting health care without insurance can have devastating consequences for an individuals budget vogenberg, 2019.