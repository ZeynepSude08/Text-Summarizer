{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3094b6-a728-47f2-8fa4-92b692e373d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1605e864-1747-4131-b717-61faabc8fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "# Tokenizer ve model (from_tf yazma!)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f5ec69a-00ad-43c1-bdd1-6ec0efe4eb07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dc63baf-6489-4ef3-b3b9-bd4e22cbccd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ea6c897-b3e7-4759-ba3e-bf7bff27731a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- ÖZET -----\n",
      "2004 film takes place in iraqi orphan community, fall saddam hussein. main character satellite, boy manages takes care orphans. Main source income unearthing landmines reselling them. end film, agrin murdered child drowning pond, commits suicide.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Model ve tokenizer'ı yükle\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Özetlenecek metni gir\n",
    "with open(\"C:/Users/Lenovo/Desktop/Metinler/turtles_summary_essay.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize et (BART için 1024 token sınırı var)\n",
    "inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Özet üret\n",
    "summary_ids = model.generate(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Tokenları yazıya çevir\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Yazdır\n",
    "print(\"----- ÖZET -----\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17208a58-1b30-42bd-8876-2170ffc12280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Özet kaydedildi: Analysis of the Shirt Scene in “The Great Gatsby” Film Essay_summary.txt\n",
      "Özet kaydedildi: Applying a Sociological Theory to the Movie ‘The Truman Show’ Essay_summary.txt\n",
      "Özet kaydedildi: Carrefour UAE Marketing Strategy Report_summary.txt\n",
      "Özet kaydedildi: Character Analysis of the Four Wives and the Maid Yan’er in the Film Raise the Red Lantern Term Paper_summary.txt\n",
      "Özet kaydedildi: Chernobyl Disaster Ethical Aspects and Effects Report_summary.txt\n",
      "Özet kaydedildi: Christian Louboutin Brand, Voice, and Guidelines Essay,_summary.txt\n",
      "Özet kaydedildi: Coca-Cola Company’s Integrated Marketing Communication Tools Report_summary.txt\n",
      "Özet kaydedildi: Computer Technology Evolution and Developments Essay_summary.txt\n",
      "Özet kaydedildi: Costco and Apple Firms’ Direct and Indirect Distribution Channels Essay_summary.txt\n",
      "Özet kaydedildi: Desire for Love (2003) Directed by Jerzy Antczak Essay (Movie Review)_summary.txt\n",
      "Özet kaydedildi: Digital Marketing Report_summary.txt\n",
      "Özet kaydedildi: Dr. Beckett’s Dental Office Service Marketing Essay_summary.txt\n",
      "Özet kaydedildi: Fire Safety Essay_summary.txt\n",
      "Özet kaydedildi: Hidden Figures Movie Summary and Analysis Essay_summary.txt\n",
      "Özet kaydedildi: Impact of Culture on Communication Reflective Essay_summary.txt\n",
      "Özet kaydedildi: Marketing Mix paper Essay_summary.txt\n",
      "Özet kaydedildi: Marriott Target Market Segmentation – Analysis Report_summary.txt\n",
      "Özet kaydedildi: McDonald’s Cultural Issues in India Research Paper_summary.txt\n",
      "Özet kaydedildi: Mental Retardation in the Movie “Forrest Gump” Term Paper_summary.txt\n",
      "Özet kaydedildi: Multinational Food Corporations & Eating Patterns in New Zealand Essay_summary.txt\n",
      "Özet kaydedildi: Object-Oriented, Event-Driven and Procedural Programming Report_summary.txt\n",
      "Özet kaydedildi: Persil Laundry Detergent Consumer-Based Segmentation Essay_summary.txt\n",
      "Özet kaydedildi: Psychological Review of the Narrator in the Movie Fight Club Essay_summary.txt\n",
      "Özet kaydedildi: Science and Technology Impact on Human Life Essay_summary.txt\n",
      "Özet kaydedildi: Sephora Company’s Marketing Strategy in the US Research Paper_summary.txt\n",
      "Özet kaydedildi: Social Media Beneficial or Harmful_summary.txt\n",
      "Özet kaydedildi: The Importance of Stakeholder Engagement for Apple Inc. Essay_summary.txt\n",
      "Özet kaydedildi: The Internet Revolution and Digital Future Technology Essay_summary.txt\n",
      "Özet kaydedildi: The Passion of Christ Analytical Essay_summary.txt\n",
      "Özet kaydedildi: The “Avatar” (2009) Film Analysis Essay_summary.txt\n",
      "Özet kaydedildi: Time Travel Is It Possible_summary.txt\n",
      "Özet kaydedildi: Toronto Blue Jays Baseball Team’s Sport Marketing Research Paper_summary.txt\n",
      "Özet kaydedildi: Turtles Can Fly Film Analysis Essay_summary.txt\n",
      "Özet kaydedildi: Walmart Problems and Solutions Essay_summary.txt\n",
      "Özet kaydedildi: “Miracle in Cell No. 7” The Representation of Mental Illness Essay_summary.txt\n",
      "Özet kaydedildi: “Parasite” by Bong Joon-Ho Essay_summary.txt\n",
      "Özet kaydedildi: “Young Sheldon” (2017) Character Case Study Report_summary.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Anahtar kelimesiz sadece özet çıkarıyor\n",
    "\n",
    "import os\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Model ve tokenizer'ı yükle\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Girdi ve çıktı klasör yolları\n",
    "input_folder = \"C:/Users/Lenovo/Desktop/Metinler/800-1500\"\n",
    "output_folder = \"C:/Users/Lenovo/Desktop/Ozetler\"\n",
    "\n",
    "# Çıktı klasörü yoksa oluştur\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Her .txt dosyası için işle\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Dosya oku\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Tokenize et\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "        # Özet üret\n",
    "        summary_ids = model.generate(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Özet dosyasını kaydet\n",
    "        summary_filename = filename.replace(\".txt\", \"_summary.txt\")\n",
    "        output_path = os.path.join(output_folder, summary_filename)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as summary_file:\n",
    "            summary_file.write(summary)\n",
    "\n",
    "        print(f\"Özet kaydedildi: {summary_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "691d4f9a-95d0-469d-b0e7-b8b6c87d9a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keybert) (1.26.4)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keybert) (13.7.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keybert) (1.6.1)\n",
      "Collecting sentence-transformers>=0.3.8 (from keybert)\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.15.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (2.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.1.31)\n",
      "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Installing collected packages: sentence-transformers, keybert\n",
      "Successfully installed keybert-0.9.0 sentence-transformers-4.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4e0bcf-9d96-4b0a-9d61-40ecaf4ecdb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Özet ve anahtar kelimeler kaydedildi: essay_1_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_10_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_100_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_101_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_102_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_103_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_104_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_105_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_106_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_107_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_108_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_109_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_11_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_110_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_111_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_112_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_113_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_114_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_115_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_116_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_117_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_118_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_119_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_12_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_120_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_121_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_122_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_123_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_124_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_125_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_126_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_127_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_128_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_129_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_13_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_130_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_131_summary.txt\n",
      "Özet ve anahtar kelimeler kaydedildi: essay_132_summary.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(summary_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Anahtar kelimeleri çıkar\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m keywords \u001b[38;5;241m=\u001b[39m kw_model\u001b[38;5;241m.\u001b[39mextract_keywords(\n\u001b[0;32m     37\u001b[0m     text, \u001b[38;5;66;03m# Özete değil, tam metne göre\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     keyphrase_ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m     39\u001b[0m     stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m,    \n\u001b[0;32m     40\u001b[0m     top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     43\u001b[0m keyword_list \u001b[38;5;241m=\u001b[39m [kw \u001b[38;5;28;01mfor\u001b[39;00m kw, score \u001b[38;5;129;01min\u001b[39;00m keywords]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Özet ve anahtar kelimeleri dosyaya yaz\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keybert\\_model.py:199\u001b[0m, in \u001b[0;36mKeyBERT.extract_keywords\u001b[1;34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords, doc_embeddings, word_embeddings, threshold)\u001b[0m\n\u001b[0;32m    197\u001b[0m     doc_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed(docs)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     word_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed(words)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Guided KeyBERT either local (keywords shared among documents) or global (keywords per document)\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed_keywords \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keybert\\backend\\_sentencetransformers.py:65\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.embed\u001b[1;34m(self, documents, verbose)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03mmatrix of embeddings.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    that each have an embeddings size of `m`\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_progress_bar\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbose})\n\u001b[1;32m---> 65\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39mencode(documents, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_kwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:685\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    682\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 685\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    687\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:758\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m    756\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m    757\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[1;32m--> 758\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:442\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[0;32m    436\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    437\u001b[0m     key: value\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    440\u001b[0m }\n\u001b[1;32m--> 442\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    443\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    444\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1144\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1144\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1145\u001b[0m     embedding_output,\n\u001b[0;32m   1146\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1147\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1148\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1149\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1150\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1151\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1152\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1153\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1154\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1155\u001b[0m )\n\u001b[0;32m   1156\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1157\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    696\u001b[0m         hidden_states,\n\u001b[0;32m    697\u001b[0m         attention_mask,\n\u001b[0;32m    698\u001b[0m         layer_head_mask,\n\u001b[0;32m    699\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    700\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    701\u001b[0m         past_key_value,\n\u001b[0;32m    702\u001b[0m         output_attentions,\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[0;32m    629\u001b[0m )\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:253\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    554\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from keybert import KeyBERT\n",
    "import torch\n",
    "\n",
    "# Model ve tokenizer'ı yükle\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# KeyBERT anahtar kelime modeli\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# Girdi ve çıktı klasör yolları\n",
    "input_folder = \"C:/Users/Lenovo/Desktop/HuggingFace_Dataset\"\n",
    "output_folder = \"C:/Users/Lenovo/Desktop/Ozetler_hug\"\n",
    "\n",
    "# Çıktı klasörü yoksa oluştur\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Her .txt dosyası için işle\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Dosya oku\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Tokenize et ve özet üret\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        summary_ids = model.generate(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Anahtar kelimeleri çıkar\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            text,                     # Özete değil, tam metne göre\n",
    "            keyphrase_ngram_range=(1, 3),\n",
    "            stop_words='english',    \n",
    "            top_n=5\n",
    "        )\n",
    "\n",
    "        keyword_list = [kw for kw, score in keywords]\n",
    "\n",
    "        # Özet ve anahtar kelimeleri dosyaya yaz\n",
    "        summary_filename = filename.replace(\".txt\", \"_summary.txt\")\n",
    "        output_path = os.path.join(output_folder, summary_filename)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as summary_file:\n",
    "            summary_file.write(\"----- ÖZET -----\\n\")\n",
    "            summary_file.write(summary + \"\\n\\n\")\n",
    "            summary_file.write(\"----- ANAHTAR KELİMELER -----\\n\")\n",
    "            summary_file.write(\", \".join(keyword_list))\n",
    "\n",
    "        print(f\"Özet ve anahtar kelimeler kaydedildi: {summary_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ed60ee4-9f0d-4191-b98a-46960bba6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Modeli yükle\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f945ee8-db1b-425e-9fe4-2c69e47b9224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77eb51f26d764d46986e105ef69dc63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396cf31088774aeca05835ae5f7d40c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9ec06628a14790af4a78eaaeaf3813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c3984cb105431aa69d777a7493ebef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b562f171c0134bbba19c0c690d7f421a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cdcdffa269f41d4afcffa6ea7da1e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kategori: media | Özet: Despite everything, the two main characters have managed to find a way to belong to one other. While Daisy and Gatsby may now feel at ease with one another, it is evident that this comfort may not continue. The audience can closely examine the sequence from the Baz Luhrmann movie and the paragraph from Fitzgerald’s book to see how the words can be brought to life.\n",
      "Kategori: social issues | Özet: The Truman Show is a drama film that captures the basic principles of the social structure at the beginning of life. The movie revolves around the life of Truman Burbank who has been in front of live cameras since before he was born without noticing. Here, Truman falls in love with Sylvia despite that the producer wanted him to marry Meryl.\n",
      "Kategori: business | Özet: Carrefour is a leading retailer that was founded in France over 40 years ago. In the last three decades, it has achieved rapid growth by joining several overseas markets such as the United Arab Emirates. The company’s success is mainly attributed to its ability to offer excellent customer service and to align its products to market needs.\n",
      "Kategori: media | Özet: In his film Raise the Red Lantern, Zhang Yimou explores various issues which have existed in the Chinese society. The five female characters develop different behavioral patterns, i.e. alienation, rebel and acceptance. Admittedly, women in the courtyard have to endure loads of constraints.\n",
      "Kategori: media | Özet: The Chernobyl disaster is one of the incidents that had never been experienced before the mid 20th century. This accident caused incalculable effects through its radiation. The staff at this nuclear power plant never acted in an egoistic way. The most effective approach to such kind of disaster was to prevent the disaster from happening.\n",
      "Kategori: history | Özet: Christian Louboutin is the shoes filled with history and associated with the perception of luxury named after the creator. The brand personality is luxury, status, and style leader. The current verbal style is not provocative while it is more peculiar with paying high attention to the catchy names of the products.\n",
      "Kategori: marketing | Özet: Integrated marketing communications play a prominent role in positive brand image creation. The Coca-Cola Company is one of the world known brands that uses integrated marketing communications as the method of its positive imagecreation. The company uses media advertising, cable TV, DVDs, direct mail, the Internet, podcasts, or special-interest magazines.\n",
      "Kategori: history | Özet: This paper examines the evolution of computers and their technology, their use in the early and modern periods, their merits and demerits, and future developments. The evolution of computer technology is divided into several generations, from mechanical devices, followed by analog devices, to the recent digital computers that now dominate the world.\n",
      "Kategori: business | Özet: Starting in 1983 with a single warehouse, Costco now operates over 700 offices worldwide. Most goods are bought directly from manufacturers and sent to consolidation points. With Kirkland’s brand, the supply chain becomes direct as it goes from the manufacturer to the end customer.\n",
      "Kategori: social issues | Özet: Desire for Love is about Frederic Chopin, a young man who moves to Paris to find a lover. He meets George Sand and falls in love with him, but later retrogresses due to tuberculosis. The film is engaging, thought-provoking, and believable because it focuses on happenings in society today.\n",
      "Kategori: marketing | Özet: The study will list and explain the five element formula for digital marketing and provide a description of two opportunities and two challenges for organisations when using digital marketing. The analysis will describe three ways organisations can learn to use digital Marketing. Lastly, the study will recommend the main reason why organisations should adopt digital Marketing and justify the preferred choice.\n",
      "Kategori: marketing | Özet: The seven Ps are utilized in marketing approaches in the evaluation and re-evaluation of business activities. In varying degrees, Dr. Beckett addresses each of the seven Ps in the reconstruction of her practice and the overall process of providing services to patients.\n",
      "Kategori: education | Özet: Recent fire incidences have revealed that most organizations do not have a fire prevention and detection program. This fire prevention essay will focus on safety measures that organizations can adopt to prevent the occurrence of fire-related incidences. Workers should handle combustible materials, such as propane, with a lot of care.\n",
      "Kategori: media | Özet: Hidden Figures (2016, directed by Theodore Melfi) is a movie that will simultaneously inspire and make people angry. The main characters of Katherine Goble, Mary Jackson, and Dorothy Vaughan worked at NASA and saw many opportunities for their professional growth. Their bosses and colleagues did not offer these women support, nor did they respect their dignity since they were all African-American. While the women’s colleagues at NASA did see the potential in them, the movie distorted some real-life events.\n",
      "Kategori: culture | Özet: The importance of culture in the success of the communication process cannot be underestimated. The cultural differences can create conflicts and misunderstanding. Culture differs from one group to another and these differences can affect the level of trust and openness in communication that one can achieve with people of other cultures.\n",
      "Kategori: marketing | Özet: The Coca-cola Company is the world’s number one manufacturer, marketer and distributor of non-alcoholic drinks. The company has managed to maintain its global leadership in the production and distribution of the soft drinks. This essay presents and explains the four elements of marketing mix; Product, Pricing, Promotion and Place.\n",
      "Kategori: marketing | Özet: The market segmentation is an important strategy in order to identify the areas in the market that can be targeted with the focus on different customers’ needs and interests. Segmenting the market, Marriott accentuates both the role of the hotel or resort’s status and its ability to respond to the customer’’ interests.\n",
      "Kategori: business | Özet: This paper looks at how businesses can be affected by cultural differences across the globe. People from different countries have different cultural orientations and preferences, which necessitate the need for multinational corporations to integrate cultural factors into their business strategy. Some of the issues which are discussed include Mcdonald’s historical background, the cultural and ethical issues at the organization's operations, and the social responsibility issues in different regions.\n",
      "Kategori: natural scicences | Özet: Mental retardation is under a group of health problem called developmental disabilities. It is generally understood as the failure of the brain to mature and grow correctly. Forrest Gump had another quality in him and it is the innocence and the graciousness of a gentleman. His disability became a stepping stone to success.\n",
      "Kategori: diet&nutrition | Özet: Eating patterns in New Zealand has deviated towards fast and unhealthy foods maintaining a great number of saturated fats, salt, and taste enhancers. Food preferences of New Zealanders have changed towards more red meat consumption, white bread, soft and energy drinks, and products containing sugar and salt.\n",
      "Kategori: design | Özet: Object-Oriented Programming (OOP) and Event-Driven Programming (EDP) are different programming paradigms. OOP uses objects and classes, while EDP relies on user interaction to determine the program flow. Objects and classes used in OOP allows for reusability of code as needed unlike Procedural Programming that requires rewriting for each instance of use.\n",
      "Kategori: business | Özet: Market segmentation is strategy that has been used and segments Persil laundry detergent Company. It differentiates itself from the other competing companies such as Aerial and Omo detergents. Some of the ways that it uses to differentiate itself is by coming up with new products which are packed in new looking packets that are acceptable by the segmented customer market.\n",
      "Kategori: media | Özet: Fight Club is a 1999 movie based on Chuck Palahniuk’s novel of the same title. In the movie, an anonymous Narrator experiences insomnia and a sense of emptiness. He meets an enigmatic and charming Tyler Durden, played by Brad Pitt, and the two men form an underground fight club that quickly gains support. The main character is a lonely and moody guy who feels cut off from his surroundings.\n",
      "Kategori: health | Özet: The Internet, smartphones, notebooks, smartwatches, and brain-medicine interfaces are used throughout the day. Despite their evident advantages in communication, data exchange, and connection, some negative impacts should not be ignored. Modern technologies facilitate human life, but health, social, and environmental outcomes remain dangerous. The Internet is a priceless contribution that helps deal with isolation and mental health challenges.\n",
      "Kategori: business | Özet: Sephora is winning the American market by giving its customers a modern experience of beauty shopping. Most of its products are designed for female customers, which can even be traced on the company’s official website. Sephora divides its main audience into four categories based on their age. The first two age groups form a population of Millennials.\n",
      "Kategori: media | Özet: Social media reshapes the social fabric, causes loss of reason, logic, attentiveness, and memory, violates individual rights of all people as well as proliferates misinformation. Unlike the internet, which brings a number of benefits, which far outweigh the harms, social media does not bring a similar imbalance in favor of good.\n",
      "Kategori: design | Özet: Apple’s success is not just the result of its cutting-edge products and exceptional design. The company is acutely aware of its role as a significant player in the global economy. It recognizes the importance of maintaining a solid and positive relationship with its internal and external stakeholders. These stakeholders include customers, employees, suppliers, shareholders, governments, and communities.\n",
      "Kategori: history | Özet: Digital Revolution refers to the change in technology that has been going on in the last 40 years. It has been characterized by rapid developments in information technology. Information technology has become part and parcel of lives as it is embedded in almost all the products. This essay explores the internet revolution and digital future technology.\n",
      "Kategori: media | Özet: In the movie, the role of Satan in the death of Jesus is exaggerated. In the bible, Satan is shown to be the one who killed Jesus. The movie shows the love of Jesus that led him to die for the sins of the world. The Bible does not show this.\n",
      "Kategori: media | Özet: James Cameron’s Avatar (2009) has a deep meaning, not just an excellent visual component. The director touched on the resounding theme of human cruelty, showing what people are ready to do for profit. The actors repeatedly use gestures and facial expressions throughout the film to show how they feel.\n",
      "Kategori: media | Özet: Time is one of the most unique and uninvestigated phenomena in our world. Discovery of wormholes also preconditioned the undying interest in the topic. There are several modern theories that could be explored to prove either the possibility or impossibility of time travels.\n",
      "Kategori: sports | Özet: Toronto Blue Jays are a Canadian baseball team that was established in the late 20th century. The team competes in Major League Baseball (MLB) and represents the American League East division. At present, the team is owned by Rogers Communications Inc. that purchased 80% of the club interests in 2000.\n",
      "Kategori: media | Özet: Turtles Can Fly is a 2004 film by Bahman Ghobadi. It is the first to be produced in Iraq after the fall of Saddam Hussein. Cinematic elements of camera angles and unsteady shots are used to communicate themes of children’s desperation and confusion in war.\n",
      "Kategori: business | Özet: Walmart is an American retail corporation established in 1962 by Sam Walton. It operates large chains of department stores and warehouses selling products at discounted prices. It has to deal with a negative reputation that stems from its low wages, low prices, and sexual exploitation. It is changing its operations strategy by converting some of its retail stores into supermarkets to offer a wider range of products.\n",
      "Kategori: media | Özet: The 2013 Korean comedy-drama movie Miracle in Cell No. 7 provides a refreshing perspective on the representation of mental illness. Lee Yong-gu, the central character of the movie, suffers from an intellectual disability. Despite his limitations, he is capable of parenting his seven-year-old daughter Ye-seung. This essay argues that the movie excelled in portraying intellectual disability but missed some important points.\n",
      "Kategori: social issues | Özet: Parasite is a critically-acclaimed film by South-Korean director Bong Joon Ho. It follows the lives of a destitute family who plot to gain employment from an affluent family and infiltrating their household. The film’s title, Parasite, is a double entendre, which leads the viewer to question who is indeed the parasite; between the rich who exploit underpaid labor, or the poor who exploit the economic resources and affluence of the rich.\n",
      "Kategori: media | Özet: Sheldon Cooper is a prominent example of a child’s unusual development pattern as presented in popular culture. The primary feature of the character stems from his unique intelligence. Sheldon attends high school from the age of nine, taking classes with his older brother, which serves to highlight his rapid intellectual development. Overall, the character of Sheldon Cooper is an exaggerated representation of aChild prodigy.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "labels = [\"art\", \"business\", \"cinematography\", \"culture\",\"design\", \"diet&nutrition\", \"economics\", \"education\", \"media\", \"environment\", \n",
    "          \"formal sciences\", \"health\", \"history\", \"law\", \"management\", \"marketing\", \"music\", \"natural scicences\", \"psyhology\", \"religion\",\n",
    "          \"social issues\", \"sociology\", \"sports\", \"engineering\", \"tourism\", \"transportation\", \"warfare\"]\n",
    "\n",
    "for summary in summaries:\n",
    "    result = classifier(summary, labels)\n",
    "    best_label = result[\"labels\"][0]\n",
    "    print(f\"Kategori: {best_label} | Özet: {summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37c96ab5-2f38-424c-b895-892a3a834cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kayıt tamamlandı.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "labels = [\"art\", \"business\", \"cinematography\", \"culture\",\"design\", \"diet&nutrition\", \"economics\", \"education\", \"media\", \"environment\", \n",
    "          \"formal sciences\", \"health\", \"history\", \"law\", \"management\", \"marketing\", \"music\", \"natural scicences\", \"psyhology\", \"religion\",\n",
    "          \"social issues\", \"sociology\", \"sports\", \"engineering\", \"tourism\", \"transportation\", \"warfare\"]\n",
    "\n",
    "# Özet + kategori listesini oluştur\n",
    "data = []\n",
    "for summary in summaries:\n",
    "    result = classifier(summary, labels)\n",
    "    best_label = result[\"labels\"][0]\n",
    "    data.append({\"summary\": summary, \"label\": best_label})\n",
    "\n",
    "# Kaydet\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"C:/Users/Lenovo/Desktop/summary_with_labels.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Kayıt tamamlandı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a7fe0e-fa7e-4b89-ac40-7727bb01a4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fca5fb761754bce93289895bbba29a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02563b0a5ec94c4bae77197ed994f634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d553783709840ba8868f4f99ae89823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  23%|##3       | 62.9M/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# Veriyi yükle\n",
    "df = pd.read_csv(\"C:/Users/Lenovo/Desktop/summary_with_labels.csv\")\n",
    "\n",
    "# Etiketleri sayısal forma çevir\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# HuggingFace Dataset'e dönüştür\n",
    "dataset = Dataset.from_pandas(df[['summary', 'label']])\n",
    "\n",
    "# Eğitim ve doğrulama olarak ayır\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "# Tokenizer ve model yükle\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['summary'], padding=True, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Modeli yükle\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Eğitim argümanları\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"C:/Users/Lenovo/Desktop/Agile_ödev_dosyaları\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Doğruluk metrik fonksiyonu\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=1)\n",
    "    accuracy = (predictions == torch.tensor(labels)).float().mean()\n",
    "    return {\"accuracy\": accuracy.item()}\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Eğitimi başlat\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddde56c-a5b7-43b4-8a01-67743d000f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# CSV dosyasını oku\n",
    "df = pd.read_csv(\"C:/Users/Lenovo/Desktop/summary_with_labels.csv\")\n",
    "\n",
    "# Eksik verileri temizle (gerekirse)\n",
    "df.dropna(subset=[\"summary\", \"label\"], inplace=True)\n",
    "\n",
    "# Eğitim ve test verilerini ayır\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"summary\"], df[\"label\"], test_size=0.3, random_state=42\n",
    ")\n",
    "#TF-IDF ile vektörleştirme\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Sınıflandırma raporu\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "yeni_metin = [\"This article is about climate change and global warming.\"]\n",
    "yeni_vec = vectorizer.transform(yeni_metin)\n",
    "print(\"Tahmin edilen kategori:\", model.predict(yeni_vec)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde0493-3923-44e6-96a4-87b2b5140637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
